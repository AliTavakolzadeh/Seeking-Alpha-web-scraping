{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from datetime import date, timedelta, datetime\n",
    "import datetime\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.\tSuccess to scrape data (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path = '/Users/alita/Desktop/chromedriver_win32/chromedriver')\n",
    "\n",
    "URL = 'https://seekingalpha.com/article/4392655-sarepta-therapeutics-inc-srpt-presents-evercore-isi-healthconx-conference-transcript'\n",
    "page1 = browser.get(URL)\n",
    "html = browser.page_source\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "participants = []\n",
    "conf_article = soup.find_all('article')\n",
    "\n",
    "for conf_elem in conf_article:\n",
    "    try:\n",
    "        title = conf_elem.find('h1').text\n",
    "    except:\n",
    "        title = 'none'\n",
    "    try:\n",
    "        txt = conf_elem.find('div', class_='sa-art article-width').next_element.text\n",
    "        txt = re.sub(\"\\s\\s+\" , \" \", txt)\n",
    "    except:\n",
    "        txt = 'none'\n",
    "        \n",
    "    try: \n",
    "        ticker = re.search('\\((.+?):(.+?)\\)', txt).group(2)\n",
    "    except:\n",
    "        ticker = 'none'\n",
    "    try:\n",
    "        company_name = re.search('(.+?)\\((.+?):', txt).group(1)\n",
    "    except:\n",
    "        company_name = 'none'\n",
    "    try:\n",
    "        date_time_str = re.search('Call (.+?) ET', txt).group(1)\n",
    "        try:\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%B %d, %Y %I:%M %p')\n",
    "        except:\n",
    "            date_time_obj = 'none'\n",
    "        try:\n",
    "            date = date_time_obj.date()\n",
    "        except:\n",
    "            date = '00-00-00'\n",
    "        try:\n",
    "            time = date_time_obj.time()\n",
    "        except:\n",
    "            time = '00:00:00'\n",
    "    except:\n",
    "        date = '00-00-00'\n",
    "        time = '00:00:00'\n",
    "        \n",
    "    # get participants\n",
    "    \n",
    "comp_participant = soup.find('p', {'class':'p p1'}, text = 'Company Participants')\n",
    "# print('Company Participants: ')\n",
    "for sibling in comp_participant.next_siblings:\n",
    "    try:\n",
    "        if(sibling.text == 'Conference Call Participants'):\n",
    "            break\n",
    "        p = sibling.text\n",
    "        name = re.search('(.+?) - (.+?)', p).group(1)\n",
    "        type_ = re.search('(.+?) - (.+)', p).group(2)\n",
    "        organization = company_name\n",
    "        participants.append({'participant_name': name, 'participant_type': type_, 'participant_organization': organization, 'conference_title': title})\n",
    "    except:\n",
    "        continue\n",
    "# print('Conference Call Participants: ')    \n",
    "conf_participant = soup.find('p', {'class':'p p1'}, text = 'Conference Call Participants')\n",
    "for sibling in conf_participant.next_siblings:\n",
    "    try:\n",
    "        p = sibling.text\n",
    "        if(sibling.find('strong')):\n",
    "            now_speech = sibling\n",
    "            break\n",
    "        name = re.search('(.+?) - (.+?)', p).group(1)\n",
    "        organization = re.search('(.+?) - (.+)', p).group(2)\n",
    "        type_ = 'guest participant'\n",
    "        participants.append({'participant_name': name, 'participant_type': type_, 'participant_organization': organization})\n",
    "    except:\n",
    "        continue\n",
    "#print(participants)\n",
    "\n",
    "#print(now_speech)\n",
    "speech = []\n",
    "speaker = now_speech.text\n",
    "section = 'Presentation'\n",
    "spoke = \" \"\n",
    "spoken_text = []\n",
    "# get speech\n",
    "for sibling in now_speech.next_siblings:\n",
    "    try:\n",
    "        if(sibling.find('strong')):\n",
    "            print(spoken_text)\n",
    "            spoke = ''.join(spoken_text)\n",
    "            speech.append({'speech_section': section, 'speech_text': spoke, 'speech_partcipant': speaker})\n",
    "            spoken_text = []\n",
    "            if(sibling.text == 'Question-and-Answer Session'):\n",
    "                section = 'Question-and-Answer'\n",
    "                continue\n",
    "            speaker = sibling.text\n",
    "            continue\n",
    "        spoken_text.append(sibling.text)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.\tSuccess to extract names and affiliations of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c.\tSuccess to extract speech for each participant. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START SCRAPING FOR DATA BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(links):\n",
    "    with open('links.txt', 'a') as f:\n",
    "        f.writelines(links)\n",
    "browser = webdriver.Chrome(executable_path = '/Users/alita/Desktop/chromedriver_win32/chromedriver')\n",
    "\n",
    "URL1 = 'https://seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "URLS = []\n",
    "URLS.append(URL1)\n",
    "for i in range(3,6):\n",
    "    URL1 = \"https://seekingalpha.com/earnings/earnings-call-transcripts/\" + str(i)\n",
    "    URLS.append(URL1)\n",
    "   \n",
    "links = []\n",
    "\n",
    "for URL in URLS: \n",
    "    page1 = browser.get(URL)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for link in soup.find_all('a', class_='dashboard-article-link'):\n",
    "        link_url = link.get('href')\n",
    "        if 'earnings-call-transcript' in link_url:\n",
    "            links.append('https://seekingalpha.com' + link_url + '\\n')\n",
    "write_to_file(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_company(row):\n",
    "    with open('company.csv', mode='w') as csv_file:\n",
    "        fieldnames = ['cy_ticker', 'cy_name']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for data in row:\n",
    "            writer.writerow(data)\n",
    "        \n",
    "def write_to_conference(row):\n",
    "    with open('conference.csv', mode='w') as csv_file:\n",
    "        fieldnames = ['cf_id', 'cf_title', 'cf_date', 'cf_time', 'Q', 'cf_year', 'cy_ticker']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for data in row:\n",
    "            writer.writerow(data)\n",
    "\n",
    "def write_to_participant(row):\n",
    "    with open('participant.csv', mode='w') as csv_file:\n",
    "        fieldnames = ['p_name', 'p_type', 'p_org', 'cf_id']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for data in row:\n",
    "            writer.writerow(data)\n",
    "\n",
    "def write_to_speech(row):\n",
    "    with open('speech.csv', mode='w') as csv_file:\n",
    "        fieldnames = ['speech_id', 'speech_section', 's_text', 'p_name', 'cf_id']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for data in row:\n",
    "            writer.writerow(data)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Chromedriver to open webpages\n",
    "browser = webdriver.Chrome(executable_path = '/Users/alita/Desktop/chromedriver_win32/chromedriver')\n",
    "\n",
    "companies = []\n",
    "conferences = []\n",
    "participants = []\n",
    "speech = []\n",
    "spch_id = 11110\n",
    "\n",
    "file1 = open('links.txt', 'r') \n",
    "\n",
    "for line in file1:   \n",
    "    URL = line\n",
    "    conf_id = re.search('https://seekingalpha.com/article/(.+?)-', URL).group(1)\n",
    "    page1 = browser.get(URL)\n",
    "    sleep(4)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    try: \n",
    "        conf_article = soup.find_all('article')\n",
    "\n",
    "        for conf_elem in conf_article:\n",
    "            try:\n",
    "                title = conf_elem.find('h1').text\n",
    "            except:\n",
    "                title = 'none'\n",
    "            try:\n",
    "                txt = conf_elem.find('div', class_='sa-art article-width').next_element.text\n",
    "                txt = re.sub(\"\\s\\s+\" , \" \", txt)\n",
    "            except:\n",
    "                txt = 'none'\n",
    "\n",
    "        \n",
    "            try: \n",
    "                q = re.search(' Q(.+?) ', txt).group(1)\n",
    "            except:\n",
    "                q = 'none'\n",
    "                \n",
    "            try: \n",
    "                year = re.search(' Q(.+?) (.+?) Earnings', txt).group(2)\n",
    "            except:\n",
    "                year = 'none'\n",
    "                \n",
    "            try: \n",
    "                ticker = re.search('\\((.+?):(.+?)\\)', txt).group(2)\n",
    "            except:\n",
    "                ticker = 'none'\n",
    "            try:\n",
    "                company_name = re.search('(.+?)\\((.+?):', txt).group(1)\n",
    "            except:\n",
    "                company_name = 'none'\n",
    "            try:\n",
    "                date_time_str = re.search('Call (.+?) ET', txt).group(1)\n",
    "                try:\n",
    "                    date_time_obj = datetime.datetime.strptime(date_time_str, '%B %d, %Y %I:%M %p')\n",
    "                except:\n",
    "                    date_time_obj = 'none'\n",
    "                try:\n",
    "                    date = date_time_obj.date()\n",
    "                except:\n",
    "                    date = '00-00-00'\n",
    "                try:\n",
    "                    time = date_time_obj.time()\n",
    "                except:\n",
    "                    time = '00:00:00'\n",
    "            except:\n",
    "                date = '00-00-00'\n",
    "                time = '00:00:00'\n",
    "            companies.append({'cy_ticker': ticker, 'cy_name': company_name})\n",
    "            conferences.append({'cf_id': conf_id, 'cf_title': title, 'cf_date': date, 'cf_time': time, 'Q': q, 'cf_year': year, 'cy_ticker': ticker})\n",
    "           \n",
    "\n",
    "        comp_participant = soup.find('p', {'class':'p p1'}, text = 'Company Participants')\n",
    "      \n",
    "        for sibling in comp_participant.next_siblings:\n",
    "            try:\n",
    "                if(sibling.text == 'Conference Call Participants'):\n",
    "                    break\n",
    "                p = sibling.text\n",
    "                name = re.search('(.+?) - (.+?)', p).group(1)\n",
    "                type_ = re.search('(.+?) - (.+)', p).group(2)\n",
    "                organization = company_name\n",
    "                participants.append({'p_name': name, 'p_type': type_, 'p_org': organization, 'cf_id': conf_id})\n",
    "            except:\n",
    "                continue\n",
    "         \n",
    "        conf_participant = soup.find('p', {'class':'p p1'}, text = 'Conference Call Participants')\n",
    "        for sibling in conf_participant.next_siblings:\n",
    "            try:\n",
    "                p = sibling.text\n",
    "                if(sibling.find('strong')):\n",
    "                    now_speech = sibling\n",
    "                    break\n",
    "                name = re.search('(.+?) - (.+?)', p).group(1)\n",
    "                organization = re.search('(.+?) - (.+)', p).group(2)\n",
    "                type_ = 'guest participant'\n",
    "                participants.append({'p_name': name, 'p_type': type_, 'p_org': organization, 'cf_id': conf_id})\n",
    "            except:\n",
    "                continue\n",
    "        # get speech\n",
    "        speaker = now_speech.text\n",
    "        section = 'Presentation'\n",
    "        for sibling in now_speech.next_siblings:\n",
    "            try:\n",
    "                if(sibling.find('strong')):\n",
    "                    if(sibling.text == 'Question-and-Answer Session'):\n",
    "                        section = 'Question-and-Answer'\n",
    "                        continue\n",
    "                    speaker = sibling.text\n",
    "                    continue\n",
    "                spoke = sibling.text\n",
    "                spch_id += 1\n",
    "                speech.append({'speech_id': spch_id, 'speech_section': section, 's_text': spoke, 'p_name': speaker, 'cf_id': conf_id})\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "write_to_company(companies)\n",
    "write_to_conference(conferences)\n",
    "write_to_participant(participants)\n",
    "write_to_speech(speech)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
